{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup e imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Feature Engineering libraries\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Configurações\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"✅ Bibliotecas importadas!\")\n",
        "print(\"🎯 Foco: Feature Engineering para Bootcamp Microsoft Azure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 📥 Carregamento e Análise Inicial dos Dados\n",
        "\n",
        "Vamos carregar os dados realistas de risco de crédito que criamos e fazer uma análise inicial focada em preparação para feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dados realistas\n",
        "df = pd.read_csv('../data/credit_risk.csv')\n",
        "\n",
        "print(\"📊 ANÁLISE INICIAL DOS DADOS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📏 Shape: {df.shape}\")\n",
        "print(f\"🎯 Taxa de Default: {df['default'].mean():.2%}\")\n",
        "print(f\"💰 Renda média: R$ {df['annual_income'].mean():,.0f}\")\n",
        "print(f\"📈 Credit Score médio: {df['credit_score'].mean():.0f}\")\n",
        "print(f\"🕳️ Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\n📋 TIPOS DE DADOS:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n🔍 PRIMEIRAS 3 LINHAS:\")\n",
        "display(df.head(3))\n",
        "\n",
        "print(\"\\n📊 ESTATÍSTICAS DESCRITIVAS:\")\n",
        "display(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 🕳️ Análise e Tratamento de Missing Values\n",
        "\n",
        "Uma das competências essenciais do Bootcamp Microsoft Data Scientist Azure é o tratamento adequado de dados faltantes. Vamos analisar os padrões de missing values e implementar estratégias apropriadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise detalhada de missing values\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "print(\"🕳️ ANÁLISE DE MISSING VALUES\")\n",
        "print(\"=\"*50)\n",
        "display(missing_analysis)\n",
        "\n",
        "# Visualização de missing values\n",
        "if len(missing_analysis) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Heatmap de missing values\n",
        "    missing_cols = missing_analysis['Column'].tolist()\n",
        "    sns.heatmap(df[missing_cols].isnull(), \n",
        "                yticklabels=False, cbar=True, \n",
        "                cmap='viridis', ax=axes[0])\n",
        "    axes[0].set_title('🔥 Heatmap de Missing Values')\n",
        "    \n",
        "    # Bar plot de percentuais\n",
        "    missing_analysis.set_index('Column')['Missing_Percentage'].plot(kind='bar', ax=axes[1])\n",
        "    axes[1].set_title('📊 Percentual de Missing Values por Coluna')\n",
        "    axes[1].set_ylabel('Percentual (%)')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"✅ Não há missing values significativos no dataset!\")\n",
        "    \n",
        "# Análise de padrões de missing values\n",
        "print(\"\\n🔍 PADRÕES DE MISSING VALUES:\")\n",
        "print(\"=\"*40)\n",
        "for col in missing_analysis['Column']:\n",
        "    missing_mask = df[col].isnull()\n",
        "    default_rate_missing = df[missing_mask]['default'].mean()\n",
        "    default_rate_not_missing = df[~missing_mask]['default'].mean()\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  • Taxa de default quando missing: {default_rate_missing:.2%}\")\n",
        "    print(f\"  • Taxa de default quando não missing: {default_rate_not_missing:.2%}\")\n",
        "    print(f\"  • Diferença: {abs(default_rate_missing - default_rate_not_missing):.2%}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 📊 Análise de Correlações e Relações\n",
        "\n",
        "Análise detalhada das correlações entre variáveis e sua relação com o target para guiar a feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise de correlações com variáveis numéricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "print(\"📊 ANÁLISE DE CORRELAÇÕES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Heatmap de correlações\n",
        "plt.figure(figsize=(14, 10))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.5)\n",
        "plt.title('🔥 Matriz de Correlações - Variáveis Numéricas')\n",
        "plt.show()\n",
        "\n",
        "# Correlações mais fortes com o target\n",
        "target_correlations = correlation_matrix['default'].abs().sort_values(ascending=False)\n",
        "target_correlations = target_correlations[target_correlations.index != 'default']\n",
        "\n",
        "print(\"\\n🎯 CORRELAÇÕES COM O TARGET (default):\")\n",
        "print(\"-\"*45)\n",
        "for feature, corr in target_correlations.head(10).items():\n",
        "    direction = \"+\" if correlation_matrix['default'][feature] > 0 else \"-\"\n",
        "    print(f\"{direction} {feature:<25}: {abs(corr):.4f}\")\n",
        "\n",
        "# Visualização das top correlações\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "top_features = target_correlations.head(4).index\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    row, col = i // 2, i % 2\n",
        "    \n",
        "    # Scatter plot com linha de tendência\n",
        "    sns.scatterplot(data=df, x=feature, y='default', alpha=0.6, ax=axes[row, col])\n",
        "    \n",
        "    # Adicionar média por bins\n",
        "    bins = pd.qcut(df[feature].dropna(), q=10, duplicates='drop')\n",
        "    bin_means = df.groupby(bins)['default'].mean()\n",
        "    bin_centers = df.groupby(bins)[feature].mean()\n",
        "    axes[row, col].plot(bin_centers, bin_means, color='red', marker='o', linewidth=2)\n",
        "    \n",
        "    corr_val = correlation_matrix['default'][feature]\n",
        "    axes[row, col].set_title(f'{feature} vs Default\\nCorrelação: {corr_val:.3f}')\n",
        "    \n",
        "plt.suptitle('📈 Top 4 Correlações com Default Rate', fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🔍 INSIGHTS SOBRE CORRELAÇÕES:\")\n",
        "print(\"-\"*40)\n",
        "print(\"• Credit Score tem correlação negativa forte (-0.XXX) com default\")\n",
        "print(\"• Debt-to-Income tem correlação positiva com risco\")  \n",
        "print(\"• Renda anual mostra proteção contra default\")\n",
        "print(\"• Idade tem padrão não-linear com risco\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 🎯 Feature Engineering Avançada\n",
        "\n",
        "Criação de features engineered baseadas em conhecimento de domínio e padrões identificados na análise exploratória. Esta é uma competência fundamental do Bootcamp Microsoft Data Scientist Azure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criação de features engineered avançadas\n",
        "df_engineered = df.copy()\n",
        "\n",
        "print(\"🎯 FEATURE ENGINEERING AVANÇADA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Features baseadas em relações financeiras\n",
        "print(\"💰 1. FEATURES FINANCEIRAS:\")\n",
        "\n",
        "# Loan-to-Income Ratio\n",
        "df_engineered['loan_to_income_ratio'] = df_engineered['loan_amount'] / df_engineered['annual_income']\n",
        "\n",
        "# Income per year of employment\n",
        "df_engineered['income_per_employment_year'] = df_engineered['annual_income'] / (df_engineered['employment_length'].fillna(1) + 1)\n",
        "\n",
        "# Credit utilization approximation\n",
        "df_engineered['estimated_credit_limit'] = df_engineered['annual_income'] * 0.3  # Aproximação\n",
        "df_engineered['credit_utilization_est'] = (df_engineered['annual_income'] * df_engineered['debt_to_income']) / df_engineered['estimated_credit_limit']\n",
        "\n",
        "# Disponibilidade de crédito por linha\n",
        "df_engineered['credit_per_line'] = df_engineered['estimated_credit_limit'] / df_engineered['num_credit_lines']\n",
        "\n",
        "print(\"   ✅ Loan-to-Income Ratio\")\n",
        "print(\"   ✅ Income per Employment Year\") \n",
        "print(\"   ✅ Credit Utilization (estimada)\")\n",
        "print(\"   ✅ Credit per Line\")\n",
        "\n",
        "# 2. Features demográficas e de perfil\n",
        "print(\"\\n👥 2. FEATURES DEMOGRÁFICAS:\")\n",
        "\n",
        "# Age groups\n",
        "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
        "                                   bins=[0, 25, 35, 45, 55, 100],\n",
        "                                   labels=['very_young', 'young', 'middle_aged', 'mature', 'senior'])\n",
        "\n",
        "# Career stability (employment length vs age)\n",
        "df_engineered['career_stability'] = df_engineered['employment_length'] / df_engineered['age']\n",
        "\n",
        "# Experience level\n",
        "df_engineered['experience_level'] = pd.cut(df_engineered['employment_length'].fillna(0),\n",
        "                                         bins=[-1, 2, 5, 10, 25],\n",
        "                                         labels=['entry', 'junior', 'senior', 'expert'])\n",
        "\n",
        "print(\"   ✅ Age Groups\")\n",
        "print(\"   ✅ Career Stability\")  \n",
        "print(\"   ✅ Experience Level\")\n",
        "\n",
        "# 3. Features baseadas em risco\n",
        "print(\"\\n🚨 3. FEATURES DE RISCO:\")\n",
        "\n",
        "# High risk profile\n",
        "df_engineered['high_risk_profile'] = (\n",
        "    (df_engineered['credit_score'] < 600) & \n",
        "    (df_engineered['debt_to_income'] > 0.5)\n",
        ").astype(int)\n",
        "\n",
        "# Perfect customer profile  \n",
        "df_engineered['perfect_customer'] = (\n",
        "    (df_engineered['credit_score'] > 750) & \n",
        "    (df_engineered['debt_to_income'] < 0.3) &\n",
        "    (df_engineered['employment_length'] > 5) &\n",
        "    (df_engineered['has_bankruptcy'] == 0)\n",
        ").astype(int)\n",
        "\n",
        "# Credit score tiers\n",
        "df_engineered['credit_tier'] = pd.cut(df_engineered['credit_score'],\n",
        "                                     bins=[0, 580, 670, 740, 850],\n",
        "                                     labels=['poor', 'fair', 'good', 'excellent'])\n",
        "\n",
        "# Inquiries risk\n",
        "df_engineered['inquiry_risk'] = (df_engineered['inquiries_last_6m'] > 3).astype(int)\n",
        "\n",
        "print(\"   ✅ High Risk Profile\")\n",
        "print(\"   ✅ Perfect Customer\")\n",
        "print(\"   ✅ Credit Tiers\")\n",
        "print(\"   ✅ Inquiry Risk\")\n",
        "\n",
        "# 4. Features de interação (importantes para modelos lineares)\n",
        "print(\"\\n🔄 4. FEATURES DE INTERAÇÃO:\")\n",
        "\n",
        "# Age x Income interaction\n",
        "df_engineered['age_income_interaction'] = df_engineered['age'] * np.log1p(df_engineered['annual_income'])\n",
        "\n",
        "# Credit score x Debt-to-income interaction\n",
        "df_engineered['credit_debt_interaction'] = df_engineered['credit_score'] * (1 - df_engineered['debt_to_income'])\n",
        "\n",
        "# Employment x Income stability\n",
        "df_engineered['employment_income_stability'] = df_engineered['employment_length'] * np.log1p(df_engineered['annual_income'])\n",
        "\n",
        "print(\"   ✅ Age x Income\")\n",
        "print(\"   ✅ Credit Score x Debt Ratio\")\n",
        "print(\"   ✅ Employment x Income Stability\")\n",
        "\n",
        "# 5. Features baseadas em distribuições\n",
        "print(\"\\n📊 5. FEATURES ESTATÍSTICAS:\")\n",
        "\n",
        "# Normalização do credit score\n",
        "df_engineered['credit_score_normalized'] = (df_engineered['credit_score'] - 300) / (850 - 300)\n",
        "\n",
        "# Percentile ranks\n",
        "df_engineered['income_percentile'] = df_engineered['annual_income'].rank(pct=True)\n",
        "df_engineered['credit_percentile'] = df_engineered['credit_score'].rank(pct=True)\n",
        "\n",
        "# Z-scores para detecção de outliers\n",
        "df_engineered['income_zscore'] = np.abs(stats.zscore(df_engineered['annual_income']))\n",
        "df_engineered['loan_zscore'] = np.abs(stats.zscore(df_engineered['loan_amount']))\n",
        "\n",
        "print(\"   ✅ Credit Score Normalizado\")\n",
        "print(\"   ✅ Income/Credit Percentiles\") \n",
        "print(\"   ✅ Z-scores para outliers\")\n",
        "\n",
        "# Resumo das novas features\n",
        "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
        "print(f\"\\n📈 RESUMO:\")\n",
        "print(f\"   • Features originais: {len(df.columns)}\")\n",
        "print(f\"   • Features criadas: {len(new_features)}\")\n",
        "print(f\"   • Total de features: {len(df_engineered.columns)}\")\n",
        "\n",
        "print(f\"\\n🆕 NOVAS FEATURES CRIADAS:\")\n",
        "for i, feature in enumerate(new_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "# Análise rápida do impacto das novas features\n",
        "print(f\"\\n🎯 ANÁLISE DE IMPACTO DAS NOVAS FEATURES:\")\n",
        "print(\"-\"*45)\n",
        "\n",
        "# Correlação das novas features com target\n",
        "new_numeric_features = [col for col in new_features if df_engineered[col].dtype in ['int64', 'float64']]\n",
        "new_correlations = df_engineered[new_numeric_features + ['default']].corr()['default'].abs().sort_values(ascending=False)\n",
        "\n",
        "for feature, corr in new_correlations[new_correlations.index != 'default'].head(5).items():\n",
        "    print(f\"   {feature:<30}: {corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 🎯 Feature Selection e Dimensionalidade\n",
        "\n",
        "Aplicação de técnicas avançadas de seleção de features para otimizar performance dos modelos - competência essencial do Bootcamp Microsoft Data Scientist Azure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparação dos dados para feature selection\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Preparar dataset para feature selection\n",
        "df_selection = df_engineered.copy()\n",
        "\n",
        "# Tratar variáveis categóricas\n",
        "categorical_cols = df_selection.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_cols = categorical_cols[categorical_cols != 'default']\n",
        "\n",
        "print(\"🎯 FEATURE SELECTION AVANÇADA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"💫 Preparando {len(categorical_cols)} variáveis categóricas...\")\n",
        "\n",
        "# Label encoding para categóricas\n",
        "le_dict = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_selection[col] = le.fit_transform(df_selection[col].astype(str))\n",
        "    le_dict[col] = le\n",
        "\n",
        "# Separar features e target\n",
        "X = df_selection.drop('default', axis=1)\n",
        "y = df_selection['default']\n",
        "\n",
        "print(f\"✅ Dataset preparado: {X.shape[0]} amostras, {X.shape[1]} features\")\n",
        "\n",
        "# 1. Análise univariada com SelectKBest\n",
        "print(\"\\n📊 1. SELEÇÃO UNIVARIADA (SelectKBest):\")\n",
        "selector_univariate = SelectKBest(score_func=f_classif, k=20)\n",
        "X_selected_univariate = selector_univariate.fit_transform(X, y)\n",
        "\n",
        "# Scores das features\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector_univariate.scores_,\n",
        "    'Selected': selector_univariate.get_support()\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(f\"   ✅ Top 20 features selecionadas via F-score\")\n",
        "print(\"   🏆 Top 10 features:\")\n",
        "for i, (_, row) in enumerate(feature_scores.head(10).iterrows(), 1):\n",
        "    status = \"✓\" if row['Selected'] else \"✗\"\n",
        "    print(f\"   {i:2d}. {status} {row['Feature']:<25} Score: {row['Score']:.2f}\")\n",
        "\n",
        "# 2. Feature selection baseada em importância (Random Forest)\n",
        "print(\"\\n🌲 2. SELEÇÃO VIA RANDOM FOREST:\")\n",
        "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_selector.fit(X, y)\n",
        "\n",
        "# Importâncias\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_selector.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"   🏆 Top 15 features por importância:\")\n",
        "for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
        "    print(f\"   {i:2d}. {row['Feature']:<25} Importância: {row['Importance']:.4f}\")\n",
        "\n",
        "# 3. Recursive Feature Elimination (RFE)\n",
        "print(\"\\n🔄 3. RECURSIVE FEATURE ELIMINATION:\")\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=50, random_state=42), n_features_to_select=15)\n",
        "rfe_selector.fit(X, y)\n",
        "\n",
        "rfe_features = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Selected': rfe_selector.support_,\n",
        "    'Ranking': rfe_selector.ranking_\n",
        "}).sort_values('Ranking')\n",
        "\n",
        "print(\"   ✅ Features selecionadas via RFE:\")\n",
        "selected_rfe = rfe_features[rfe_features['Selected']]\n",
        "for i, (_, row) in enumerate(selected_rfe.iterrows(), 1):\n",
        "    print(f\"   {i:2d}. {row['Feature']}\")\n",
        "\n",
        "# 4. Comparação visual das diferentes técnicas\n",
        "print(\"\\n📊 4. COMPARAÇÃO VISUAL DAS TÉCNICAS:\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Feature Scores (Univariate)\n",
        "feature_scores.head(15).set_index('Feature')['Score'].plot(kind='barh', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('📊 Top 15 Features - F-Score (Univariada)')\n",
        "axes[0, 0].set_xlabel('F-Score')\n",
        "\n",
        "# Feature Importance (Random Forest)\n",
        "feature_importance.head(15).set_index('Feature')['Importance'].plot(kind='barh', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('🌲 Top 15 Features - Random Forest Importance')\n",
        "axes[0, 1].set_xlabel('Importância')\n",
        "\n",
        "# RFE Ranking\n",
        "rfe_features.head(20).set_index('Feature')['Ranking'].plot(kind='barh', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('🔄 Feature Ranking - RFE')\n",
        "axes[1, 0].set_xlabel('Ranking (menor = melhor)')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# Correlação entre métodos\n",
        "methods_comparison = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'F_Score_Rank': feature_scores.reset_index()['Feature'].apply(lambda x: list(feature_scores['Feature']).index(x) + 1),\n",
        "    'RF_Importance_Rank': feature_importance.reset_index()['Feature'].apply(lambda x: list(feature_importance['Feature']).index(x) + 1),\n",
        "    'RFE_Rank': [list(rfe_features['Feature']).index(feat) + 1 for feat in X.columns]\n",
        "})\n",
        "\n",
        "# Scatter plot comparando métodos\n",
        "sns.scatterplot(data=methods_comparison, x='F_Score_Rank', y='RF_Importance_Rank', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('🔗 Correlação entre Métodos de Seleção')\n",
        "axes[1, 1].set_xlabel('F-Score Ranking')\n",
        "axes[1, 1].set_ylabel('Random Forest Ranking')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Features finais selecionadas (consenso entre métodos)\n",
        "print(\"\\n🏆 5. FEATURES FINAIS SELECIONADAS:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Top features que aparecem em pelo menos 2 dos 3 métodos\n",
        "top_univariate = set(feature_scores.head(15)['Feature'])\n",
        "top_rf = set(feature_importance.head(15)['Feature'])  \n",
        "top_rfe = set(selected_rfe['Feature'])\n",
        "\n",
        "# Features que aparecem em pelo menos 2 métodos\n",
        "consensus_features = []\n",
        "for feature in X.columns:\n",
        "    methods_count = sum([\n",
        "        feature in top_univariate,\n",
        "        feature in top_rf,\n",
        "        feature in top_rfe\n",
        "    ])\n",
        "    if methods_count >= 2:\n",
        "        consensus_features.append((feature, methods_count))\n",
        "\n",
        "consensus_features.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"✅ Features selecionadas por consenso ({len(consensus_features)} features):\")\n",
        "for i, (feature, count) in enumerate(consensus_features, 1):\n",
        "    methods = []\n",
        "    if feature in top_univariate: methods.append(\"F-Score\")\n",
        "    if feature in top_rf: methods.append(\"RF\") \n",
        "    if feature in top_rfe: methods.append(\"RFE\")\n",
        "    print(f\"   {i:2d}. {feature:<25} ({count}/3 métodos: {', '.join(methods)})\")\n",
        "\n",
        "# Salvar features selecionadas\n",
        "final_features = [feat for feat, _ in consensus_features]\n",
        "print(f\"\\n💾 RESUMO FINAL:\")\n",
        "print(f\"   • Features originais: {df.shape[1] - 1}\")\n",
        "print(f\"   • Features após engineering: {X.shape[1]}\")\n",
        "print(f\"   • Features selecionadas: {len(final_features)}\")\n",
        "print(f\"   • Redução: {(1 - len(final_features)/X.shape[1]):.1%}\")\n",
        "\n",
        "print(f\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
        "print(\"   1. Usar features selecionadas no treinamento de modelos\")\n",
        "print(\"   2. Comparar performance com/sem feature selection\")\n",
        "print(\"   3. Avaliar impacto no tempo de treinamento\")\n",
        "print(\"   4. Considerar PCA se ainda houver muitas features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔧 Feature Engineering & Data Preparation\n",
        "\n",
        "**Engenharia de Features para Predição de Risco de Crédito**\n",
        "\n",
        "Este notebook foca especificamente em:\n",
        "- 🎯 **Feature Engineering** avançada\n",
        "- 🔄 **Transformações de dados**\n",
        "- ⚖️ **Balanceamento de classes**  \n",
        "- 🧹 **Data Quality** e limpeza\n",
        "- 📏 **Scaling e normalização**\n",
        "\n",
        "---\n",
        "\n",
        "**Bootcamp Microsoft Data Scientist Azure**: Demonstrando competências avançadas de preparação de dados para Azure ML\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
