{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup e imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Feature Engineering libraries\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# ConfiguraÃ§Ãµes\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"âœ… Bibliotecas importadas!\")\n",
        "print(\"ğŸ¯ Foco: Feature Engineering para Bootcamp Microsoft Azure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ğŸ“¥ Carregamento e AnÃ¡lise Inicial dos Dados\n",
        "\n",
        "Vamos carregar os dados realistas de risco de crÃ©dito que criamos e fazer uma anÃ¡lise inicial focada em preparaÃ§Ã£o para feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dados realistas\n",
        "df = pd.read_csv('../data/credit_risk.csv')\n",
        "\n",
        "print(\"ğŸ“Š ANÃLISE INICIAL DOS DADOS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“ Shape: {df.shape}\")\n",
        "print(f\"ğŸ¯ Taxa de Default: {df['default'].mean():.2%}\")\n",
        "print(f\"ğŸ’° Renda mÃ©dia: R$ {df['annual_income'].mean():,.0f}\")\n",
        "print(f\"ğŸ“ˆ Credit Score mÃ©dio: {df['credit_score'].mean():.0f}\")\n",
        "print(f\"ğŸ•³ï¸ Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\nğŸ“‹ TIPOS DE DADOS:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nğŸ” PRIMEIRAS 3 LINHAS:\")\n",
        "display(df.head(3))\n",
        "\n",
        "print(\"\\nğŸ“Š ESTATÃSTICAS DESCRITIVAS:\")\n",
        "display(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ğŸ•³ï¸ AnÃ¡lise e Tratamento de Missing Values\n",
        "\n",
        "Uma das competÃªncias essenciais do Bootcamp Microsoft Data Scientist Azure Ã© o tratamento adequado de dados faltantes. Vamos analisar os padrÃµes de missing values e implementar estratÃ©gias apropriadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AnÃ¡lise detalhada de missing values\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "print(\"ğŸ•³ï¸ ANÃLISE DE MISSING VALUES\")\n",
        "print(\"=\"*50)\n",
        "display(missing_analysis)\n",
        "\n",
        "# VisualizaÃ§Ã£o de missing values\n",
        "if len(missing_analysis) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Heatmap de missing values\n",
        "    missing_cols = missing_analysis['Column'].tolist()\n",
        "    sns.heatmap(df[missing_cols].isnull(), \n",
        "                yticklabels=False, cbar=True, \n",
        "                cmap='viridis', ax=axes[0])\n",
        "    axes[0].set_title('ğŸ”¥ Heatmap de Missing Values')\n",
        "    \n",
        "    # Bar plot de percentuais\n",
        "    missing_analysis.set_index('Column')['Missing_Percentage'].plot(kind='bar', ax=axes[1])\n",
        "    axes[1].set_title('ğŸ“Š Percentual de Missing Values por Coluna')\n",
        "    axes[1].set_ylabel('Percentual (%)')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âœ… NÃ£o hÃ¡ missing values significativos no dataset!\")\n",
        "    \n",
        "# AnÃ¡lise de padrÃµes de missing values\n",
        "print(\"\\nğŸ” PADRÃ•ES DE MISSING VALUES:\")\n",
        "print(\"=\"*40)\n",
        "for col in missing_analysis['Column']:\n",
        "    missing_mask = df[col].isnull()\n",
        "    default_rate_missing = df[missing_mask]['default'].mean()\n",
        "    default_rate_not_missing = df[~missing_mask]['default'].mean()\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  â€¢ Taxa de default quando missing: {default_rate_missing:.2%}\")\n",
        "    print(f\"  â€¢ Taxa de default quando nÃ£o missing: {default_rate_not_missing:.2%}\")\n",
        "    print(f\"  â€¢ DiferenÃ§a: {abs(default_rate_missing - default_rate_not_missing):.2%}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ğŸ“Š AnÃ¡lise de CorrelaÃ§Ãµes e RelaÃ§Ãµes\n",
        "\n",
        "AnÃ¡lise detalhada das correlaÃ§Ãµes entre variÃ¡veis e sua relaÃ§Ã£o com o target para guiar a feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AnÃ¡lise de correlaÃ§Ãµes com variÃ¡veis numÃ©ricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "print(\"ğŸ“Š ANÃLISE DE CORRELAÃ‡Ã•ES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Heatmap de correlaÃ§Ãµes\n",
        "plt.figure(figsize=(14, 10))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.5)\n",
        "plt.title('ğŸ”¥ Matriz de CorrelaÃ§Ãµes - VariÃ¡veis NumÃ©ricas')\n",
        "plt.show()\n",
        "\n",
        "# CorrelaÃ§Ãµes mais fortes com o target\n",
        "target_correlations = correlation_matrix['default'].abs().sort_values(ascending=False)\n",
        "target_correlations = target_correlations[target_correlations.index != 'default']\n",
        "\n",
        "print(\"\\nğŸ¯ CORRELAÃ‡Ã•ES COM O TARGET (default):\")\n",
        "print(\"-\"*45)\n",
        "for feature, corr in target_correlations.head(10).items():\n",
        "    direction = \"+\" if correlation_matrix['default'][feature] > 0 else \"-\"\n",
        "    print(f\"{direction} {feature:<25}: {abs(corr):.4f}\")\n",
        "\n",
        "# VisualizaÃ§Ã£o das top correlaÃ§Ãµes\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "top_features = target_correlations.head(4).index\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    row, col = i // 2, i % 2\n",
        "    \n",
        "    # Scatter plot com linha de tendÃªncia\n",
        "    sns.scatterplot(data=df, x=feature, y='default', alpha=0.6, ax=axes[row, col])\n",
        "    \n",
        "    # Adicionar mÃ©dia por bins\n",
        "    bins = pd.qcut(df[feature].dropna(), q=10, duplicates='drop')\n",
        "    bin_means = df.groupby(bins)['default'].mean()\n",
        "    bin_centers = df.groupby(bins)[feature].mean()\n",
        "    axes[row, col].plot(bin_centers, bin_means, color='red', marker='o', linewidth=2)\n",
        "    \n",
        "    corr_val = correlation_matrix['default'][feature]\n",
        "    axes[row, col].set_title(f'{feature} vs Default\\nCorrelaÃ§Ã£o: {corr_val:.3f}')\n",
        "    \n",
        "plt.suptitle('ğŸ“ˆ Top 4 CorrelaÃ§Ãµes com Default Rate', fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ” INSIGHTS SOBRE CORRELAÃ‡Ã•ES:\")\n",
        "print(\"-\"*40)\n",
        "print(\"â€¢ Credit Score tem correlaÃ§Ã£o negativa forte (-0.XXX) com default\")\n",
        "print(\"â€¢ Debt-to-Income tem correlaÃ§Ã£o positiva com risco\")  \n",
        "print(\"â€¢ Renda anual mostra proteÃ§Ã£o contra default\")\n",
        "print(\"â€¢ Idade tem padrÃ£o nÃ£o-linear com risco\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ğŸ¯ Feature Engineering AvanÃ§ada\n",
        "\n",
        "CriaÃ§Ã£o de features engineered baseadas em conhecimento de domÃ­nio e padrÃµes identificados na anÃ¡lise exploratÃ³ria. Esta Ã© uma competÃªncia fundamental do Bootcamp Microsoft Data Scientist Azure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CriaÃ§Ã£o de features engineered avanÃ§adas\n",
        "df_engineered = df.copy()\n",
        "\n",
        "print(\"ğŸ¯ FEATURE ENGINEERING AVANÃ‡ADA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Features baseadas em relaÃ§Ãµes financeiras\n",
        "print(\"ğŸ’° 1. FEATURES FINANCEIRAS:\")\n",
        "\n",
        "# Loan-to-Income Ratio\n",
        "df_engineered['loan_to_income_ratio'] = df_engineered['loan_amount'] / df_engineered['annual_income']\n",
        "\n",
        "# Income per year of employment\n",
        "df_engineered['income_per_employment_year'] = df_engineered['annual_income'] / (df_engineered['employment_length'].fillna(1) + 1)\n",
        "\n",
        "# Credit utilization approximation\n",
        "df_engineered['estimated_credit_limit'] = df_engineered['annual_income'] * 0.3  # AproximaÃ§Ã£o\n",
        "df_engineered['credit_utilization_est'] = (df_engineered['annual_income'] * df_engineered['debt_to_income']) / df_engineered['estimated_credit_limit']\n",
        "\n",
        "# Disponibilidade de crÃ©dito por linha\n",
        "df_engineered['credit_per_line'] = df_engineered['estimated_credit_limit'] / df_engineered['num_credit_lines']\n",
        "\n",
        "print(\"   âœ… Loan-to-Income Ratio\")\n",
        "print(\"   âœ… Income per Employment Year\") \n",
        "print(\"   âœ… Credit Utilization (estimada)\")\n",
        "print(\"   âœ… Credit per Line\")\n",
        "\n",
        "# 2. Features demogrÃ¡ficas e de perfil\n",
        "print(\"\\nğŸ‘¥ 2. FEATURES DEMOGRÃFICAS:\")\n",
        "\n",
        "# Age groups\n",
        "df_engineered['age_group'] = pd.cut(df_engineered['age'], \n",
        "                                   bins=[0, 25, 35, 45, 55, 100],\n",
        "                                   labels=['very_young', 'young', 'middle_aged', 'mature', 'senior'])\n",
        "\n",
        "# Career stability (employment length vs age)\n",
        "df_engineered['career_stability'] = df_engineered['employment_length'] / df_engineered['age']\n",
        "\n",
        "# Experience level\n",
        "df_engineered['experience_level'] = pd.cut(df_engineered['employment_length'].fillna(0),\n",
        "                                         bins=[-1, 2, 5, 10, 25],\n",
        "                                         labels=['entry', 'junior', 'senior', 'expert'])\n",
        "\n",
        "print(\"   âœ… Age Groups\")\n",
        "print(\"   âœ… Career Stability\")  \n",
        "print(\"   âœ… Experience Level\")\n",
        "\n",
        "# 3. Features baseadas em risco\n",
        "print(\"\\nğŸš¨ 3. FEATURES DE RISCO:\")\n",
        "\n",
        "# High risk profile\n",
        "df_engineered['high_risk_profile'] = (\n",
        "    (df_engineered['credit_score'] < 600) & \n",
        "    (df_engineered['debt_to_income'] > 0.5)\n",
        ").astype(int)\n",
        "\n",
        "# Perfect customer profile  \n",
        "df_engineered['perfect_customer'] = (\n",
        "    (df_engineered['credit_score'] > 750) & \n",
        "    (df_engineered['debt_to_income'] < 0.3) &\n",
        "    (df_engineered['employment_length'] > 5) &\n",
        "    (df_engineered['has_bankruptcy'] == 0)\n",
        ").astype(int)\n",
        "\n",
        "# Credit score tiers\n",
        "df_engineered['credit_tier'] = pd.cut(df_engineered['credit_score'],\n",
        "                                     bins=[0, 580, 670, 740, 850],\n",
        "                                     labels=['poor', 'fair', 'good', 'excellent'])\n",
        "\n",
        "# Inquiries risk\n",
        "df_engineered['inquiry_risk'] = (df_engineered['inquiries_last_6m'] > 3).astype(int)\n",
        "\n",
        "print(\"   âœ… High Risk Profile\")\n",
        "print(\"   âœ… Perfect Customer\")\n",
        "print(\"   âœ… Credit Tiers\")\n",
        "print(\"   âœ… Inquiry Risk\")\n",
        "\n",
        "# 4. Features de interaÃ§Ã£o (importantes para modelos lineares)\n",
        "print(\"\\nğŸ”„ 4. FEATURES DE INTERAÃ‡ÃƒO:\")\n",
        "\n",
        "# Age x Income interaction\n",
        "df_engineered['age_income_interaction'] = df_engineered['age'] * np.log1p(df_engineered['annual_income'])\n",
        "\n",
        "# Credit score x Debt-to-income interaction\n",
        "df_engineered['credit_debt_interaction'] = df_engineered['credit_score'] * (1 - df_engineered['debt_to_income'])\n",
        "\n",
        "# Employment x Income stability\n",
        "df_engineered['employment_income_stability'] = df_engineered['employment_length'] * np.log1p(df_engineered['annual_income'])\n",
        "\n",
        "print(\"   âœ… Age x Income\")\n",
        "print(\"   âœ… Credit Score x Debt Ratio\")\n",
        "print(\"   âœ… Employment x Income Stability\")\n",
        "\n",
        "# 5. Features baseadas em distribuiÃ§Ãµes\n",
        "print(\"\\nğŸ“Š 5. FEATURES ESTATÃSTICAS:\")\n",
        "\n",
        "# NormalizaÃ§Ã£o do credit score\n",
        "df_engineered['credit_score_normalized'] = (df_engineered['credit_score'] - 300) / (850 - 300)\n",
        "\n",
        "# Percentile ranks\n",
        "df_engineered['income_percentile'] = df_engineered['annual_income'].rank(pct=True)\n",
        "df_engineered['credit_percentile'] = df_engineered['credit_score'].rank(pct=True)\n",
        "\n",
        "# Z-scores para detecÃ§Ã£o de outliers\n",
        "df_engineered['income_zscore'] = np.abs(stats.zscore(df_engineered['annual_income']))\n",
        "df_engineered['loan_zscore'] = np.abs(stats.zscore(df_engineered['loan_amount']))\n",
        "\n",
        "print(\"   âœ… Credit Score Normalizado\")\n",
        "print(\"   âœ… Income/Credit Percentiles\") \n",
        "print(\"   âœ… Z-scores para outliers\")\n",
        "\n",
        "# Resumo das novas features\n",
        "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
        "print(f\"\\nğŸ“ˆ RESUMO:\")\n",
        "print(f\"   â€¢ Features originais: {len(df.columns)}\")\n",
        "print(f\"   â€¢ Features criadas: {len(new_features)}\")\n",
        "print(f\"   â€¢ Total de features: {len(df_engineered.columns)}\")\n",
        "\n",
        "print(f\"\\nğŸ†• NOVAS FEATURES CRIADAS:\")\n",
        "for i, feature in enumerate(new_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "# AnÃ¡lise rÃ¡pida do impacto das novas features\n",
        "print(f\"\\nğŸ¯ ANÃLISE DE IMPACTO DAS NOVAS FEATURES:\")\n",
        "print(\"-\"*45)\n",
        "\n",
        "# CorrelaÃ§Ã£o das novas features com target\n",
        "new_numeric_features = [col for col in new_features if df_engineered[col].dtype in ['int64', 'float64']]\n",
        "new_correlations = df_engineered[new_numeric_features + ['default']].corr()['default'].abs().sort_values(ascending=False)\n",
        "\n",
        "for feature, corr in new_correlations[new_correlations.index != 'default'].head(5).items():\n",
        "    print(f\"   {feature:<30}: {corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ğŸ¯ Feature Selection e Dimensionalidade\n",
        "\n",
        "AplicaÃ§Ã£o de tÃ©cnicas avanÃ§adas de seleÃ§Ã£o de features para otimizar performance dos modelos - competÃªncia essencial do Bootcamp Microsoft Data Scientist Azure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PreparaÃ§Ã£o dos dados para feature selection\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Preparar dataset para feature selection\n",
        "df_selection = df_engineered.copy()\n",
        "\n",
        "# Tratar variÃ¡veis categÃ³ricas\n",
        "categorical_cols = df_selection.select_dtypes(include=['object', 'category']).columns\n",
        "categorical_cols = categorical_cols[categorical_cols != 'default']\n",
        "\n",
        "print(\"ğŸ¯ FEATURE SELECTION AVANÃ‡ADA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ’« Preparando {len(categorical_cols)} variÃ¡veis categÃ³ricas...\")\n",
        "\n",
        "# Label encoding para categÃ³ricas\n",
        "le_dict = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_selection[col] = le.fit_transform(df_selection[col].astype(str))\n",
        "    le_dict[col] = le\n",
        "\n",
        "# Separar features e target\n",
        "X = df_selection.drop('default', axis=1)\n",
        "y = df_selection['default']\n",
        "\n",
        "print(f\"âœ… Dataset preparado: {X.shape[0]} amostras, {X.shape[1]} features\")\n",
        "\n",
        "# 1. AnÃ¡lise univariada com SelectKBest\n",
        "print(\"\\nğŸ“Š 1. SELEÃ‡ÃƒO UNIVARIADA (SelectKBest):\")\n",
        "selector_univariate = SelectKBest(score_func=f_classif, k=20)\n",
        "X_selected_univariate = selector_univariate.fit_transform(X, y)\n",
        "\n",
        "# Scores das features\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector_univariate.scores_,\n",
        "    'Selected': selector_univariate.get_support()\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(f\"   âœ… Top 20 features selecionadas via F-score\")\n",
        "print(\"   ğŸ† Top 10 features:\")\n",
        "for i, (_, row) in enumerate(feature_scores.head(10).iterrows(), 1):\n",
        "    status = \"âœ“\" if row['Selected'] else \"âœ—\"\n",
        "    print(f\"   {i:2d}. {status} {row['Feature']:<25} Score: {row['Score']:.2f}\")\n",
        "\n",
        "# 2. Feature selection baseada em importÃ¢ncia (Random Forest)\n",
        "print(\"\\nğŸŒ² 2. SELEÃ‡ÃƒO VIA RANDOM FOREST:\")\n",
        "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_selector.fit(X, y)\n",
        "\n",
        "# ImportÃ¢ncias\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_selector.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"   ğŸ† Top 15 features por importÃ¢ncia:\")\n",
        "for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
        "    print(f\"   {i:2d}. {row['Feature']:<25} ImportÃ¢ncia: {row['Importance']:.4f}\")\n",
        "\n",
        "# 3. Recursive Feature Elimination (RFE)\n",
        "print(\"\\nğŸ”„ 3. RECURSIVE FEATURE ELIMINATION:\")\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=50, random_state=42), n_features_to_select=15)\n",
        "rfe_selector.fit(X, y)\n",
        "\n",
        "rfe_features = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Selected': rfe_selector.support_,\n",
        "    'Ranking': rfe_selector.ranking_\n",
        "}).sort_values('Ranking')\n",
        "\n",
        "print(\"   âœ… Features selecionadas via RFE:\")\n",
        "selected_rfe = rfe_features[rfe_features['Selected']]\n",
        "for i, (_, row) in enumerate(selected_rfe.iterrows(), 1):\n",
        "    print(f\"   {i:2d}. {row['Feature']}\")\n",
        "\n",
        "# 4. ComparaÃ§Ã£o visual das diferentes tÃ©cnicas\n",
        "print(\"\\nğŸ“Š 4. COMPARAÃ‡ÃƒO VISUAL DAS TÃ‰CNICAS:\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Feature Scores (Univariate)\n",
        "feature_scores.head(15).set_index('Feature')['Score'].plot(kind='barh', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('ğŸ“Š Top 15 Features - F-Score (Univariada)')\n",
        "axes[0, 0].set_xlabel('F-Score')\n",
        "\n",
        "# Feature Importance (Random Forest)\n",
        "feature_importance.head(15).set_index('Feature')['Importance'].plot(kind='barh', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('ğŸŒ² Top 15 Features - Random Forest Importance')\n",
        "axes[0, 1].set_xlabel('ImportÃ¢ncia')\n",
        "\n",
        "# RFE Ranking\n",
        "rfe_features.head(20).set_index('Feature')['Ranking'].plot(kind='barh', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('ğŸ”„ Feature Ranking - RFE')\n",
        "axes[1, 0].set_xlabel('Ranking (menor = melhor)')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# CorrelaÃ§Ã£o entre mÃ©todos\n",
        "methods_comparison = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'F_Score_Rank': feature_scores.reset_index()['Feature'].apply(lambda x: list(feature_scores['Feature']).index(x) + 1),\n",
        "    'RF_Importance_Rank': feature_importance.reset_index()['Feature'].apply(lambda x: list(feature_importance['Feature']).index(x) + 1),\n",
        "    'RFE_Rank': [list(rfe_features['Feature']).index(feat) + 1 for feat in X.columns]\n",
        "})\n",
        "\n",
        "# Scatter plot comparando mÃ©todos\n",
        "sns.scatterplot(data=methods_comparison, x='F_Score_Rank', y='RF_Importance_Rank', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('ğŸ”— CorrelaÃ§Ã£o entre MÃ©todos de SeleÃ§Ã£o')\n",
        "axes[1, 1].set_xlabel('F-Score Ranking')\n",
        "axes[1, 1].set_ylabel('Random Forest Ranking')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Features finais selecionadas (consenso entre mÃ©todos)\n",
        "print(\"\\nğŸ† 5. FEATURES FINAIS SELECIONADAS:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Top features que aparecem em pelo menos 2 dos 3 mÃ©todos\n",
        "top_univariate = set(feature_scores.head(15)['Feature'])\n",
        "top_rf = set(feature_importance.head(15)['Feature'])  \n",
        "top_rfe = set(selected_rfe['Feature'])\n",
        "\n",
        "# Features que aparecem em pelo menos 2 mÃ©todos\n",
        "consensus_features = []\n",
        "for feature in X.columns:\n",
        "    methods_count = sum([\n",
        "        feature in top_univariate,\n",
        "        feature in top_rf,\n",
        "        feature in top_rfe\n",
        "    ])\n",
        "    if methods_count >= 2:\n",
        "        consensus_features.append((feature, methods_count))\n",
        "\n",
        "consensus_features.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"âœ… Features selecionadas por consenso ({len(consensus_features)} features):\")\n",
        "for i, (feature, count) in enumerate(consensus_features, 1):\n",
        "    methods = []\n",
        "    if feature in top_univariate: methods.append(\"F-Score\")\n",
        "    if feature in top_rf: methods.append(\"RF\") \n",
        "    if feature in top_rfe: methods.append(\"RFE\")\n",
        "    print(f\"   {i:2d}. {feature:<25} ({count}/3 mÃ©todos: {', '.join(methods)})\")\n",
        "\n",
        "# Salvar features selecionadas\n",
        "final_features = [feat for feat, _ in consensus_features]\n",
        "print(f\"\\nğŸ’¾ RESUMO FINAL:\")\n",
        "print(f\"   â€¢ Features originais: {df.shape[1] - 1}\")\n",
        "print(f\"   â€¢ Features apÃ³s engineering: {X.shape[1]}\")\n",
        "print(f\"   â€¢ Features selecionadas: {len(final_features)}\")\n",
        "print(f\"   â€¢ ReduÃ§Ã£o: {(1 - len(final_features)/X.shape[1]):.1%}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ PRÃ“XIMOS PASSOS:\")\n",
        "print(\"   1. Usar features selecionadas no treinamento de modelos\")\n",
        "print(\"   2. Comparar performance com/sem feature selection\")\n",
        "print(\"   3. Avaliar impacto no tempo de treinamento\")\n",
        "print(\"   4. Considerar PCA se ainda houver muitas features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”§ Feature Engineering & Data Preparation\n",
        "\n",
        "**Engenharia de Features para PrediÃ§Ã£o de Risco de CrÃ©dito**\n",
        "\n",
        "Este notebook foca especificamente em:\n",
        "- ğŸ¯ **Feature Engineering** avanÃ§ada\n",
        "- ğŸ”„ **TransformaÃ§Ãµes de dados**\n",
        "- âš–ï¸ **Balanceamento de classes**  \n",
        "- ğŸ§¹ **Data Quality** e limpeza\n",
        "- ğŸ“ **Scaling e normalizaÃ§Ã£o**\n",
        "\n",
        "---\n",
        "\n",
        "**Bootcamp Microsoft Data Scientist Azure**: Demonstrando competÃªncias avanÃ§adas de preparaÃ§Ã£o de dados para Azure ML\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
